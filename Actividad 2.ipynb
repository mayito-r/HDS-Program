{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2\n",
    "## Tema 6-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imayz\\.virtualenvs\\hds-zbVMaGVs\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] No se puede encontrar el m칩dulo especificado\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# tama침o batch, arbitrario\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tensor es una matriz multidimensional n x m x k, similar a aun array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranformaciones apllicadas a las imagenes, imagen -> tensor\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear train y test sets.\n",
    "num_workers = 0, windows no soporta >0 para cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)\n",
    "\n",
    "## download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtner primer batch\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensiones del tensor\n",
    "# batch x canal x alto x ancho\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer imagen del bacth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x241378a9dc8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "primera = images[0].numpy()\n",
    "primera = np.transpose(primera, (1, 2, 0))\n",
    "plt.imshow(primera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicion del modelo\n",
    "modelo con 1 capa convolucional con kernel de tama침o 3 y 2 tranformacione lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 28x28x1 => 26x26x32\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                              out_channels=32,\n",
    "                              kernel_size=3)\n",
    "        # tama침o del kernel ?\n",
    "        # out chanel 32?\n",
    "\n",
    "        self.d1 = nn.Linear(26 * 26 * 32, 128)\n",
    "        self.d2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 32x1x28x28 => 32x32x26x26\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # funcion de activacion\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # flatten => 32 x (32*26*26)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "\n",
    "        # 32 x (32*26*26) => 32x128\n",
    "        x = self.d1(x)\n",
    "        # funcion de activacion d1\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # logits => 32x10\n",
    "        logits = self.d2(x)\n",
    "\n",
    "        # salida softmax\n",
    "        out = F.softmax(logits, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyModel()\n",
    "\n",
    "# modelo CNN \n",
    "model = model.to(device)\n",
    "\n",
    "## Parametros del modelo ---\n",
    "# lr y veces que se entrena\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "# funcion de perdida\n",
    "# CrossEntropyLoss para clasificaion multiclase\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# algortimo de optimizacion\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para calcular precision \n",
    "def get_accuracy(logit, target, batch_size):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.6252 | Train Accuracy: 83.96\n",
      "Epoch: 1 | Loss: 1.4910 | Train Accuracy: 97.30\n",
      "Epoch: 2 | Loss: 1.4816 | Train Accuracy: 98.16\n",
      "Epoch: 3 | Loss: 1.4771 | Train Accuracy: 98.60\n",
      "Epoch: 4 | Loss: 1.4739 | Train Accuracy: 98.91\n"
     ]
    }
   ],
   "source": [
    "# iterar sobre epochs\n",
    "for epoch in range(num_epochs):\n",
    "    train_running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    ## training step\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        ## forward + backprop + loss\n",
    "        \n",
    "        # scores\n",
    "        logits = model(images)\n",
    "        # perdida\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ## update model params\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.detach().item()\n",
    "        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss / i, train_acc/i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.90\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0.0\n",
    "for i, (images, labels) in enumerate(testloader, 0):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n",
    "\n",
    "print('Test Accuracy: %.2f'%( test_acc/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar modelo localmente\n",
    "# torch.save(model.state_dict(), \"D:/Descargas/HDS/Modulo 6/Semana 14/61/trained_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar prediccion sobre los dos primeros elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = iter(testloader)\n",
    "images_test, labels_test = test_image.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probar sobre dos imagenes\n",
    "images_test[0:2].shape\n",
    "y_pred = model(images_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = torch.max(y_pred, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor predicciones: [7 2]\n",
      "valores reales: [7 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valor predicciones:\", max_index.data.numpy().squeeze())\n",
    "print(\"valores reales:\",  labels_test[0:2].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones \n",
    "Me sorprendi칩 el nivel de precisi칩n que alcanzo en modelo con este data set, el tutorial es relativamente sencillo de seguir, logre entrenar la red e incluso probar la red para ver los valores de las predicciones sobre algunos elementos del set de test y comprobar que los resultados eran acertados.\n",
    "\n",
    "Considero que este tutorial tiene cierto grado de complejidad en la parte te칩rica, ya que omite varios conceptos, y requiere un entendimiento previo de como funciona una CNN y otros conceptos. He visto varios tutoriales similares, y creo que tienen a sobre simplificar el tema, lo que puede crear interpretaci칩n err칩nea del uso de redes y por ende una mala aplicaci칩n de estas.\n",
    "Este tutorial creo m치s bien sirve para demostrar la facilidad de uso de pytorch, a칰n as칤 no queda del todo claro el uso de los m칩dulos de  la librer칤a, tuve que leer directamente la documentaci칩n de pytorch para comprender su uso. \n",
    "\n",
    "Como conclusi칩n esta actividad motiva a querer adentrase m치s en uso de modelos de DL, con las bases te칩ricas y t칠cnicas es relativamente sencillo llevar a la pr치ctica su aplicaci칩n.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c75bb5de08ed7297136e0cc7b8d7bce837d4cb4816fa8e967ad71c53874eb3d1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('hds-zbVMaGVs': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
