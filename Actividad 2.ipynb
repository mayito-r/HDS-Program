{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2\n",
    "## Tema 6-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imayz\\.virtualenvs\\hds-zbVMaGVs\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] No se puede encontrar el módulo especificado\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# tamaño batch, arbitrario\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tensor es una matriz multidimensional n x m x k, similar a aun array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranformaciones apllicadas a las imagenes, imagen -> tensor\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear train y test sets.\n",
    "num_workers = 0, windows no soporta >0 para cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)\n",
    "\n",
    "## download and load testing dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtner primer batch\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensiones del tensor\n",
    "# batch x canal x alto x ancho\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer imagen del bacth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x241378a9dc8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "primera = images[0].numpy()\n",
    "primera = np.transpose(primera, (1, 2, 0))\n",
    "plt.imshow(primera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicion del modelo\n",
    "modelo con 1 capa convolucional con kernel de tamaño 3 y 2 tranformacione lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 28x28x1 => 26x26x32\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                              out_channels=32,\n",
    "                              kernel_size=3)\n",
    "        # tamaño del kernel ?\n",
    "        # out chanel 32?\n",
    "\n",
    "        self.d1 = nn.Linear(26 * 26 * 32, 128)\n",
    "        self.d2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 32x1x28x28 => 32x32x26x26\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # funcion de activacion\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # flatten => 32 x (32*26*26)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "\n",
    "        # 32 x (32*26*26) => 32x128\n",
    "        x = self.d1(x)\n",
    "        # funcion de activacion d1\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # logits => 32x10\n",
    "        logits = self.d2(x)\n",
    "\n",
    "        # salida softmax\n",
    "        out = F.softmax(logits, dim=1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyModel()\n",
    "\n",
    "# modelo CNN \n",
    "model = model.to(device)\n",
    "\n",
    "## Parametros del modelo ---\n",
    "# lr y veces que se entrena\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "# funcion de perdida\n",
    "# CrossEntropyLoss para clasificaion multiclase\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# algortimo de optimizacion\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para calcular precision \n",
    "def get_accuracy(logit, target, batch_size):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "    accuracy = 100.0 * corrects/batch_size\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.6252 | Train Accuracy: 83.96\n",
      "Epoch: 1 | Loss: 1.4910 | Train Accuracy: 97.30\n",
      "Epoch: 2 | Loss: 1.4816 | Train Accuracy: 98.16\n",
      "Epoch: 3 | Loss: 1.4771 | Train Accuracy: 98.60\n",
      "Epoch: 4 | Loss: 1.4739 | Train Accuracy: 98.91\n"
     ]
    }
   ],
   "source": [
    "# iterar sobre epochs\n",
    "for epoch in range(num_epochs):\n",
    "    train_running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    ## training step\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        ## forward + backprop + loss\n",
    "        \n",
    "        # scores\n",
    "        logits = model(images)\n",
    "        # perdida\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ## update model params\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.detach().item()\n",
    "        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' %(epoch, train_running_loss / i, train_acc/i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.90\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0.0\n",
    "for i, (images, labels) in enumerate(testloader, 0):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n",
    "\n",
    "print('Test Accuracy: %.2f'%( test_acc/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar modelo localmente\n",
    "# torch.save(model.state_dict(), \"D:/Descargas/HDS/Modulo 6/Semana 14/61/trained_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar prediccion sobre los dos primeros elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = iter(testloader)\n",
    "images_test, labels_test = test_image.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probar sobre dos imagenes\n",
    "images_test[0:2].shape\n",
    "y_pred = model(images_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = torch.max(y_pred, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor predicciones: [7 2]\n",
      "valores reales: [7 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valor predicciones:\", max_index.data.numpy().squeeze())\n",
    "print(\"valores reales:\",  labels_test[0:2].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones \n",
    "Me sorprendió el nivel de precisión que alcanzo en modelo con este data set, el tutorial es relativamente sencillo de seguir, logre entrenar la red e incluso probar la red para ver los valores de las predicciones sobre algunos elementos del set de test y comprobar que los resultados eran acertados.\n",
    "\n",
    "Considero que este tutorial tiene cierto grado de complejidad en la parte teórica, ya que omite varios conceptos, y requiere un entendimiento previo de como funciona una CNN y otros conceptos. He visto varios tutoriales similares, y creo que tienen a sobre simplificar el tema, lo que puede crear interpretación errónea del uso de redes y por ende una mala aplicación de estas.\n",
    "Este tutorial creo más bien sirve para demostrar la facilidad de uso de pytorch, aún así no queda del todo claro el uso de los módulos de  la librería, tuve que leer directamente la documentación de pytorch para comprender su uso. \n",
    "\n",
    "Como conclusión esta actividad motiva a querer adentrase más en uso de modelos de DL, con las bases teóricas y técnicas es relativamente sencillo llevar a la práctica su aplicación.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c75bb5de08ed7297136e0cc7b8d7bce837d4cb4816fa8e967ad71c53874eb3d1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('hds-zbVMaGVs': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
